@INPROCEEDINGS{falsone2014randomized,
  author = {A. Falsone and F. Noce and M. Prandini},
  title = {A randomized approach to space debris footprint characterization},
  booktitle = {Proceedings of 19th World Congress of the International Federation
	of Automatic Control (IFAC 2014), Cape Town},
  year = {2014},
  url = {http://www.nt.ntnu.no/users/skoge/prost/proceedings/ifac2014/media/files/0612.pdf},
abstract={Although a substantial amount of research has examined the constructs of warmth and competence, far less has examined how these constructs develop and what benefits may accrue when warmth and competence are cultivated. Yet there are positive consequences, both emotional and behavioral, that are likely to occur when brands hold perceptions of both. In this paper, we shed light on when and how warmth and competence are jointly promoted in brands, and why these reputations matter.}
}
@INPROCEEDINGS{falsone2014novel,
  author = {A. Falsone and L. Piroddi and M. Prandini},
  title = {A novel randomized approach to nonlinear system identification},
  booktitle = {Proceedings of the 53rd Conference on Decision and Control (CDC 2014),
	Los Angeles},
  year = {2014},
abstract={Although a substantial amount of research has examined the constructs of warmth and competence, far less has examined how these constructs develop and what benefits may accrue when warmth and competence are cultivated. Yet there are positive consequences, both emotional and behavioral, that are likely to occur when brands hold perceptions of both. In this paper, we shed light on when and how warmth and competence are jointly promoted in brands, and why these reputations matter.}
}
@article{Xarticle,
    author    = {A. Falsone and M. Pirotta},
    title     = {This is a journal},
    journal   = {Topolino},
    year      = {2015},
    url       = {http://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf},
abstract={Although a substantial amount of research has examined the constructs of warmth and competence, far less has examined how these constructs develop and what benefits may accrue when warmth and competence are cultivated. Yet there are positive consequences, both emotional and behavioral, that are likely to occur when brands hold perceptions of both. In this paper, we shed light on when and how warmth and competence are jointly promoted in brands, and why these reputations matter.}
}
@book{goossens93,
    author    = "Michel Goossens and Frank Mittelbach and Alexander Samarin",
    title     = "The LaTeX Companion",
    year      = "1993",
    publisher = "Addison-Wesley",
    abstract  = "Reading, Massachusetts"
}
@INPROCEEDINGS{Migliavacca2011fitted,
  author = {Martino Migliavacca and Alessio Pecorino and Matteo Pirotta and Marcello
	Restelli and Andrea Bonarini},
  title = {Fitted policy search},
  booktitle = {2011 {IEEE} Symposium on Adaptive Dynamic Programming And Reinforcement
	Learning, {ADPRL} 2011, Paris, France, April 12-14, 2011},
  year = {2011},
  pages = {287--294},
  publisher = {{IEEE}},
  abstract = {In this paper we address the combination of batch reinforcement-learning
	(BRL) techniques with direct policy search (DPS) algorithms in the
	context of robot learning. Batch value-based algorithms (such as
	fitted Q-iteration) have been proved to outperform online ones in
	many complex applications, but they share the same difficulties in
	solving problems with continuous action spaces, such as robotic ones.
	In these cases, actor-critic and DPS methods are preferable, since
	the optimization process is limited to a family of parameterized
	(usually smooth) policies. On the other hand, these methods (e.g.,
	policy gradient and evolutionary methods) are generally very expensive,
	since finding the optimal parameterization may require to evaluate
	the performance of several policies, which in many real robotic applications
	is unfeasible or even dangerous. To overcome such problems, we exploit
	the fitted policy search (FPS) approach, in which the expected return
	of any policy considered during the optimization process is evaluated
	offline (without resorting to the robot) by reusing the data collected
	in the initial exploration phase. In this way, it is possible to
	take the advantages of both BRL and DPS algorithms, thus achieving
	an effective learning approach to solve robotic problems. A balancing
	task on a real two-wheeled robotic pendulum is used to analyze the
	properties and evaluate the effectiveness of the FPS approach.},
  doi = {10.1109/ADPRL.2011.5967368},
  url = {http://dx.doi.org/10.1109/ADPRL.2011.5967368}
}

@INPROCEEDINGS{Migliavacca2010fitted,
  author = {Martino Migliavacca and Alessio Pecorino and Matteo Pirotta and Marcello
	Restelli and Andrea Bonarini},
  title = {Fitted Policy Search: Direct Policy Search using a Batch Reinforcement
	Learning Approach},
  booktitle = {Proceedings of the 19th European Conference on Artificial Intelligence
	(ECAI 2010): 3rd International Workshop on Evolutionary and Reinforcement
	Learning for Autonomous Robot Systems (ERLARS)},
  year = {2010},
  pages = {35--42},
  abstract = {In this paper we address the combination of batch reinforcement-learning
	(BRL) techniques with direct policy search (DPS) algorithms in the
	context of robot learning. Batch value-based algorithms (such as
	fitted Q-iteration) have been proved to outperform online ones in
	many complex applications, but they share the same difficulties in
	solving problems with continuous action spaces, such as robotic ones.
	In these cases, actor-critic and DPS methods
	
	are preferable, since the optimization process is limited to a family
	of parameterized (usually smooth) policies. On the other hand, these
	methods (e.g., policy gradient and evolutionary methods) are generally
	very expensive, since finding the optimal parameterization may require
	to evaluate the performance of several policies, which in many real
	robotic applications is unfeasible or even dangerous. To overcome
	such problems, we propose the fitted policy search (FPS) approach,
	in which the expected return of any policy considered during the
	optimization process is evaluated offline (without resorting to the
	robot) by reusing the data collected in the initial exploration phase.
	In this way, it is possible to take the advantages of both BRL and
	DPS algorithms, thus achieving an effective learning approach to
	solve robotic problems. A balancing task on a real two-wheeled robotic
	pendulum is used to analyze the properties and evaluate the effectiveness
	of the proposed approach.}
}

@INPROCEEDINGS{Parisi2014morlgradient,
  author = {Simone Parisi and Matteo Pirotta and Nicola Smacchia and Luca Bascetta
	and Marcello Restelli},
  title = {Policy gradient approaches for multi-objective sequential decision
	making},
  booktitle = {2014 International Joint Conference on Neural Networks, {IJCNN} 2014,
	Beijing, China, July 6-11, 2014},
  year = {2014},
  pages = {2323--2330},
  publisher = {{IEEE}},
  abstract = {This paper investigates the use of policy gradient techniques to approximate
	the Pareto frontier in Multi-Objective Markov Decision Processes
	(MOMDPs). Despite the popularity of policy gradient algorithms and
	the fact that gradient ascent algorithms have been already proposed
	to numerically solve multi-objective optimization problems, especially
	in combination with multi-objective evolutionary algorithms, so far
	little attention has been paid to the use of gradient information
	to face multi-objective sequential decision problems. Two different
	Multi-Objective Reinforcement-Learning (MORL) approaches, called
	radial and Pareto following, that, starting from an initial policy,
	perform gradient-based policy-search procedures aimed at finding
	a set of non-dominated policies are here presented. Both algorithms
	are empirically evaluated and compared to state-of-the-art MORL algorithms
	on three MORL benchmark problems.},
  doi = {10.1109/IJCNN.2014.6889738},
  url = {http://dx.doi.org/10.1109/IJCNN.2014.6889738}
}

@INPROCEEDINGS{pirotta2014particle,
  author = {Pirotta, Matteo and Manganini, Giorgio and Piroddi, Luigi and Prandini,
	Maria and Restelli, Marcello},
  title = {A Particle-Based Policy for the Optimal Control of Markov Decision
	Processes},
  booktitle = {The 19th IFAC World Congress},
  year = {2014},
  volume = {19},
  number = {1},
  pages = {10518--10523},
  abstract = {When the state dimension is large, classical approximate dynamic programming
	techniques may become computationally unfeasible, since the complexity
	of the algorithm grows exponentially with the state space size (curse
	of dimensionality). Policy search techniques are able to overcome
	this problem because, instead of estimating the value function over
	the entire state space, they search for the optimal control policy
	in a restricted parameterized policy space. This paper presents a
	new policy parametrization that exploits a single point (particle)
	to represent an entire region of the state space and can be tuned
	through a recently introduced policy gradient method with parameter-based
	exploration. Experiments demonstrate the superior performance of
	the proposed approach in high dimensional environments.},
  url = {http://www.nt.ntnu.no/users/skoge/prost/proceedings/ifac2014/media/files/1987.pdf}
}